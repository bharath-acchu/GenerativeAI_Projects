{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d7b8c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d1bf2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93e55dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Star\\anaconda3\\envs\\llmapp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from git import Repo\n",
    "from langchain.text_splitter import Language\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings , GoogleGenerativeAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1eb878c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Star\\\\Desktop\\\\github\\\\Github\\\\GenAI\\\\GenerativeAI_Projects\\\\SourceCodeAnalyser\\\\research'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f94c056",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir test_repo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9ba068e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<git.repo.base.Repo 'c:\\\\Users\\\\Star\\\\Desktop\\\\github\\\\Github\\\\GenAI\\\\GenerativeAI_Projects\\\\SourceCodeAnalyser\\\\research\\\\test_repo\\\\.git'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_path = \"test_repo/\"\n",
    "\n",
    "Repo.clone_from(\"https://github.com/bharath-acchu/GenerativeAI_Projects\", to_path=repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc6f50e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = \"test_repo/\"\n",
    "\n",
    "loader = GenericLoader.from_filesystem(repo_path,\n",
    "                                        glob = \"**/*\",\n",
    "                                       suffixes=[\".py\"],\n",
    "                                       parser = LanguageParser(language=Language.PYTHON, parser_threshold=500)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e341ae1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'test_repo\\\\End_To_End_MedicalChatBot\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='from flask import Flask, render_template,jsonify,request\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom langchain_google_genai import GoogleGenerativeAI\\nfrom langchain.chains import create_retrieval_chain\\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom src.helper import getGoogleEmbeddings\\nfrom src.prompt import *\\nimport os\\nfrom dotenv import load_dotenv\\n\\napp = Flask(__name__)\\nload_dotenv()\\n\\nPINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\\nos.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\\nGOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\\nos.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\\n\\nembeddings = getGoogleEmbeddings()\\nindex_name = \"medicalbot\"\\ndocsearch = PineconeVectorStore.from_existing_index(index_name=index_name,embedding=embeddings)\\n\\nretriever = docsearch.as_retriever(search_type=\"similarity\",search_kwargs={\"k\":3})\\n\\nllm = GoogleGenerativeAI(model=\"models/gemini-2.0-flash\")\\n\\nprompt = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", system_prompt),\\n         (\"human\", \"{input}\")]\\n)\\n\\nquestion_answer_chain = create_stuff_documents_chain(llm,prompt)\\nrag_chain = create_retrieval_chain(retriever,question_answer_chain)\\n\\n@app.route(\"/\")\\ndef index():\\n    return render_template(\\'chat.html\\')\\n\\n@app.route(\"/get\",methods=[\"GET\",\"POST\"])\\ndef chat():\\n    msg = request.form[\\'msg\\']\\n    input = msg\\n    print(input)\\n    response = rag_chain.invoke({\"input\":msg})\\n    print(\"Response : \", response[\"answer\"])\\n    return str(response[\"answer\"])\\n\\n\\nif __name__ == \\'__main__\\':\\n    app.run(host=\"0.0.0.0\", port=8080, debug=True)\\n\\n\\n\\n\\n\\n\\n'),\n",
       " Document(metadata={'source': 'test_repo\\\\End_To_End_MedicalChatBot\\\\setup.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"from setuptools import find_packages, setup\\n\\nsetup(\\n    name = 'Generative AI Project',\\n    version = '0.0.0',\\n    author = 'Bharath',\\n    author_email='bharath.sdupuc@gmail.com',\\n    packages=find_packages(),\\n    install_requires=[]\\n)\"),\n",
       " Document(metadata={'source': 'test_repo\\\\End_To_End_MedicalChatBot\\\\store_index.py', 'language': <Language.PYTHON: 'python'>}, page_content='from src.helper import load_pdf,text_split,getGoogleEmbeddings\\nfrom pinecone.grpc import PineconeGRPC as Pinecone\\nfrom pinecone import ServerlessSpec\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom dotenv import load_dotenv\\nload_dotenv()\\nimport os\\n\\nPINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\\nos.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\\nGOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\\nos.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\\n\\nextracted_data = load_pdf(data=\\'Data/\\')\\ntext_chunks = text_split(extracted_data)\\nembeddings = getGoogleEmbeddings()\\n\\npc = Pinecone(api_key=PINECONE_API_KEY)\\n\\nindex_name = \"medicalbot\"\\n\\npc.create_index(\\n    name=index_name,\\n    dimension=768, # Replace with your model dimensions\\n    metric=\"cosine\", # Replace with your model metric\\n    spec=ServerlessSpec(\\n        cloud=\"aws\",\\n        region=\"us-east-1\"\\n    ) \\n)\\n\\n\\ndef batch(iterable, batch_size=500):\\n    for i in range(0, len(iterable), batch_size):\\n        yield iterable[i:i + batch_size]\\n\\nfor chunk_batch in batch(text_chunks, batch_size=500):\\n    PineconeVectorStore.from_documents(\\n        documents=chunk_batch,\\n        index_name=index_name,\\n        embedding=embeddings\\n    )\\n\\nprint(\"âœ… Uploaded safely in manual batches!\")\\n'),\n",
       " Document(metadata={'source': 'test_repo\\\\End_To_End_MedicalChatBot\\\\template.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nfrom pathlib import Path\\nimport logging\\n\\nlogging.basicConfig(level=logging.INFO, format=\\'[%(asctime)s]: %(message)s\\')\\n\\n\\nlist_of_files = [\\n    \"src/__init__.py\",\\n    \"src/helper.py\",\\n    \"src/prompt.py\",\\n    \".env\",\\n    \"setup.py\",\\n    \"app.py\",\\n    \"research/trials.ipynb\"\\n]\\n\\n\\nfor filepath in list_of_files:\\n    filepath = Path(filepath)\\n    filedir, filename = os.path.split(filepath)\\n\\n    if filedir !=\"\":\\n        os.makedirs(filedir, exist_ok=True)\\n        logging.info(f\"Creating directory; {filedir} for the file: {filename}\")\\n\\n    if(not os.path.exists(filepath)) or (os.path.getsize(filepath) == 0):\\n        with open(filepath, \"w\") as f:\\n            pass\\n            logging.info(f\"Creating empty file: {filepath}\")\\n    else:\\n        logging.info(f\"{filename} is already exists\")'),\n",
       " Document(metadata={'source': 'test_repo\\\\End_To_End_MedicalChatBot\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}, page_content='from langchain.document_loaders import PyPDFLoader, DirectoryLoader\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nimport google.generativeai as genai\\nfrom langchain_google_genai import GoogleGenerativeAI, GoogleGenerativeAIEmbeddings\\n\\n\\n# extract data from pdf files\\ndef load_pdf(data):\\n    loader = DirectoryLoader(\\n        data,\\n        glob=\"*.pdf\",\\n        loader_cls=PyPDFLoader\\n        )\\n    documents = loader.load()\\n\\n    return documents\\n\\n# split the data into chunks\\ndef text_split(extracted_data):\\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=450,chunk_overlap=20)\\n    text_chunks = text_splitter.split_documents(extracted_data)\\n    return text_chunks\\n\\n# define and return the text embedding model\\ndef getGoogleEmbeddings():\\n    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\\n    return embeddings\\n\\n\\n'),\n",
       " Document(metadata={'source': 'test_repo\\\\End_To_End_MedicalChatBot\\\\src\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}, page_content='system_prompt = \"\"\"You are an assistant for question answering tasks.\\nUse the following pieces of retrieved context to answer the question.\\nIf you don\\'t know the answer, just say that you don\\'t know.\\nUse three sentences maximum and keep the answer concise.\\n\\n\\n{context}\"\"\"'),\n",
       " Document(metadata={'source': 'test_repo\\\\End_To_End_MedicalChatBot\\\\src\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content='')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd5184d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the chunks 6\n"
     ]
    }
   ],
   "source": [
    "#chunking\n",
    "\n",
    "documents_splitter = RecursiveCharacterTextSplitter.from_language(language = Language.PYTHON,\n",
    "                                                             chunk_size = 2000,\n",
    "                                                             chunk_overlap = 200)\n",
    "\n",
    "text_chunks = documents_splitter.split_documents(documents)\n",
    "print(\"Length of the chunks\",len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20bdac4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12cfae1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model = 'models/text-embedding-004')\n",
    "llm = GoogleGenerativeAI(model=\"models/gemini-2.0-flash\", temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d15b15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Star\\AppData\\Local\\Temp\\ipykernel_5820\\2863379319.py:6: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "# VECTOR DB\n",
    "\n",
    "\n",
    "\n",
    "vectordb = Chroma.from_documents(text_chunks, embedding=embeddings, persist_directory='./db')\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db261822",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Star\\AppData\\Local\\Temp\\ipykernel_5820\\15224368.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryMemory(llm=llm, memory_key = \"chat_history\", return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationSummaryMemory(llm=llm, memory_key = \"chat_history\", return_messages=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2095ee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mmr - Maximal Marginal Relevance - \n",
    "# Instead of just retrieving documents purely by similarity to the query, MMR tries to balance two things:\n",
    "# -->  Relevance to the query\n",
    "# --> Diversity among the retrieved documents\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever=vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\":3}), memory=memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8eca421",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is load_pdf do?\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a651b96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Star\\AppData\\Local\\Temp\\ipykernel_5820\\79176006.py:1: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_pdf extracts data from PDF files located in a directory. It uses DirectoryLoader with PyPDFLoader to load all PDF files specified by the glob pattern \"*.pdf\". The function returns a list of Document objects.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = qa(question)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a7c88c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ques = \"what exactly getGoogleEmbeddings in helper.py ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32cb1852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `getGoogleEmbeddings` function in `helper.py` is defined as:\n",
      "\n",
      "```python\n",
      "def getGoogleEmbeddings():\n",
      "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
      "    return embeddings\n",
      "```\n",
      "\n",
      "It initializes and returns a `GoogleGenerativeAIEmbeddings` object, using the \"models/text-embedding-004\" model. This object is used for creating embeddings of text.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = qa.invoke(ques)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd457a84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmapp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
